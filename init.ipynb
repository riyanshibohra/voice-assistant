{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a voice assistant for knowledge base!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Whisper, OpenAI, Eleven Labs, and ActiveLoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "ELEVEN_API_KEY = os.getenv(\"ELEVEN_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Create or get index\n",
    "index_name = \"voice-assistant\"\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,  # OpenAI embeddings dimension\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Initialize the Pinecone vector store\n",
    "vectorstore = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    "    text_key=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Web scraping content ( Python library articles ) from Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the documentation URLs\n",
    "\n",
    "def get_documentation_urls():\n",
    "\n",
    "    return[\n",
    "        \t'/docs/huggingface_hub/guides/overview',\n",
    "\t\t    '/docs/huggingface_hub/guides/download',\n",
    "\t\t    '/docs/huggingface_hub/guides/upload',\n",
    "\t\t    '/docs/huggingface_hub/guides/hf_file_system',\n",
    "\t\t    '/docs/huggingface_hub/guides/repository',\n",
    "\t\t    '/docs/huggingface_hub/guides/search',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct the full URL\n",
    "\n",
    "def construct_full_url(base_url, relative_url):\n",
    "    return base_url + relative_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_content(url):\n",
    "    response = requests.get(url)    #get request to the url\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')    #use beautiful soup to parse the html content\n",
    "    text = soup.body.text.strip()     # Extract the desired content from the page (in this case, the body text)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)    # Remove any whitespace characters\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all content from the given URLs and save to a file\n",
    "\n",
    "def scrape_all_content(base_url,relative_urls,filename):\n",
    "\n",
    "    content = []\n",
    "    for i in relative_urls:\n",
    "        full_url = construct_full_url(base_url,i)\n",
    "        scraped_content = scrape_page_content(full_url)\n",
    "        content.append(scraped_content.rstrip('\\n'))\n",
    "\n",
    "    # Save the content to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for item in content:\n",
    "            file.write(\"%s\\n\" % item)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load documents from a file\n",
    "\n",
    "def load_docs(root_dir,filename):\n",
    "    docs = []\n",
    "    try:\n",
    "        loader = TextLoader(os.path.join(root_dir,filename), encoding='utf-8')\n",
    "        docs.extend(loader.load_and_split())\n",
    "    except Exception as e:\n",
    "        pass      #if an error occurs, pass it and continue\n",
    "    return docs\n",
    "\n",
    "def split_docs(docs):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap = 0)\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Embedding and storing in Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content scraped, embedded, and stored in Pinecone successfully!\n"
     ]
    }
   ],
   "source": [
    "# define the main function\n",
    "\n",
    "def main():\n",
    "    base_url = 'https://huggingface.co'\n",
    "    # Set the name of the file to which the scraped content will be saved\n",
    "    filename='content.txt'\n",
    "    # Set the root directory where the content file will be saved\n",
    "    root_dir ='./'\n",
    "    relative_urls = get_documentation_urls()\n",
    "\n",
    "    content = scrape_all_content(base_url,relative_urls,filename)\n",
    "\n",
    "    docs = load_docs(root_dir,filename)\n",
    "\n",
    "    texts = split_docs(docs)\n",
    "\n",
    "    # Extract text content and create IDs\n",
    "    text_contents = [doc.page_content for doc in texts]\n",
    "    doc_ids = [f\"doc_{i}\" for i in range(len(texts))]\n",
    "\n",
    "    # Add documents to vector store with IDs\n",
    "    vectorstore.add_texts(\n",
    "        texts=text_contents,\n",
    "        ids=doc_ids,\n",
    "        metadatas=[doc.metadata for doc in texts]\n",
    "    )\n",
    "\n",
    "    os.remove(filename)\n",
    "\n",
    "    print(\"Content scraped, embedded, and stored in Pinecone successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Voice Assistant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import streamlit as st\n",
    "from audio_recorder_streamlit import audio_recorder\n",
    "from elevenlabs import generate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from streamlit_chat import message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TEMP_AUDIO_PATH = \"temp_audio.wav\"\n",
    "AUDIO_FORMAT = \"audio/wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file and return the keys\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "eleven_api_key = os.environ.get('ELEVEN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load embeddings and database\n",
    "\n",
    "def load_embeddings_and_database():\n",
    "    # Initialize embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # Initialize Pinecone\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    # Get the existing index\n",
    "    db = PineconeVectorStore(\n",
    "        index=pc.Index(\"voice-assistant\"),\n",
    "        embedding=embeddings,\n",
    "        text_key=\"text\"\n",
    "    )\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes an audio file path and OpenAI API key as parameters\n",
    "\n",
    "def transcribe_audio(audio_file_path,openai_key):\n",
    "\n",
    "    # Set the OpenAI API key for authentication\n",
    "    openai.api_key = openai_key\n",
    "    try:\n",
    "        # Open the audio file in binary read mode\n",
    "        with open(audio_file_path,\"rb\") as audio_file:\n",
    "            # Call OpenAI's Whisper API to transcribe the audio file\n",
    "            response = openai.Audio.transcribe(\"whisper-1\",audio_file)       # Uses whisper-1 model and passes the audio file\n",
    "        # Return just the transcribed text from the response\n",
    "        return response[\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Whisper API: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code transcribes an audio file into text using the OpenAI Whisper API, requiring the path of the audio file and the OpenAI key as input parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record audio using audio_recorder and transcribe using transcribe_audio\n",
    "\n",
    "def record_and_transcribe_audio():\n",
    "    # Record audio from the microphone\n",
    "    audio_bytes = audio_recorder()\n",
    "    transcription = None\n",
    "    if audio_bytes:\n",
    "        st.audio(audio_bytes,format=AUDIO_FORMAT)\n",
    "\n",
    "        with open(TEMP_AUDIO_PATH,\"wb\") as f:\n",
    "            f.write(audio_bytes)\n",
    "\n",
    "        if st.button(\"Transcribe\"):\n",
    "            transcription = transcribe_audio(TEMP_AUDIO_PATH,openai.api_key)\n",
    "            os.remove(TEMP_AUDIO_PATH)\n",
    "            display_transcription(transcription)\n",
    "\n",
    "        return transcription\n",
    "    \n",
    "# Display the transcription of the audio on the app\n",
    "\n",
    "def display_transcription(transcription):\n",
    "    if transcription:\n",
    "        st.write(f\"Transcription: {transcription}\")\n",
    "        with open(\"audio_transcription.txt\", \"w+\") as f:\n",
    "            f.write(transcription)\n",
    "    else:\n",
    "        st.write(\"Error transcribing audio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
